{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ycn_yRLpeW0W",
    "outputId": "0101bfed-c52b-4380-b71c-15f5314d73da"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Veri kümesi\n",
    "dataset = pd.read_csv('dataset.csv', names = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Tweet'], encoding='latin1')\n",
    "\n",
    "\n",
    "# Toplamda 6 sütundan oluşan veri kümesinde yaklaşık 1.6 milyon tweet bulunduğu doğrulanmıştır.\n",
    "print(\"Veri kümesindeki toplam tweet sayısı: {} Milyon\".format(dataset.shape[0]/1000000.0))\n",
    "\n",
    "# Sütun tipleri ve eksik veri durumu `info()` metodu ile analiz edilmiştir.\n",
    "dataset.info()\n",
    "\n",
    "\n",
    "## Column analysis\n",
    "# Sütunlar analiz edilmiştir.\n",
    "\n",
    "### Sentiment\n",
    "# Sentiment (duygu) sütunundaki dağılım incelenmiştir.\n",
    "\n",
    "# Pozitif ve negatif tweet’lerin sayıları `value_counts()` ile sayılmıştır.\n",
    "class_count = dataset['Sentiment'].value_counts()  # Sıralı şekilde [4, 0]\n",
    "\n",
    "# Pozitif ve negatif duygu durumlarının dağılımı çubuk grafik ile görselleştirilmiştir.\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.xticks([4, 0], ['Pozitif', 'Negatif'])\n",
    "plt.bar(x = class_count.keys(),\n",
    "        height = class_count.values,\n",
    "        color = ['g', 'r'])\n",
    "plt.xlabel(\"Tweet Duygusu\")\n",
    "plt.ylabel(\"Tweet Sayısı\")\n",
    "plt.title(\"Her duygu türü için tweet sayısı\")\n",
    "plt.legend()\n",
    "\n",
    "# Verinin pozitif (4) ve negatif (0) tweet’ler açısından dengeli olduğu gözlemlenmiştir.\n",
    "\n",
    "\n",
    "### Date\n",
    "# Tarih bilgisi kullanılarak aylara göre tweet dağılımı analiz edilmiştir.\n",
    "\n",
    "# Ay bilgisi `Date` sütunundan ayrıştırılmıştır.\n",
    "dataset['Month'] = dataset['Date'].apply(lambda date: date.split(' ')[1])\n",
    "months_count = dataset['Month'].value_counts()\n",
    "\n",
    "# Aylara göre tweet sayısı çubuk grafik ile görselleştirilmiştir.\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.bar(['Jun', 'May', 'Apr'], months_count.values, color = ['b', 'g', 'r'])\n",
    "for i, v in enumerate(months_count.values):\n",
    "    plt.text(i - 0.1, v + 10000, str(v))\n",
    "plt.xlabel('Aylar')\n",
    "plt.ylabel('Tweet Sayısı')\n",
    "plt.title('2009 yılı aylarına göre tweet dağılımı')\n",
    "\n",
    "# En fazla tweet’in Haziran 2009’da atıldığı tespit edilmiştir.\n",
    "\n",
    "\n",
    "### Id, Flag and User\n",
    "# `Id`, `Flag` ve `User` sütunlarının duygu analizi için faydasız olduğu değerlendirilmiş ve bu sütunlar kullanılmayacaktır.\n",
    "\n",
    "\n",
    "### Tweet\n",
    "# Tweet içeriği incelenmiştir.\n",
    "\n",
    "# İlk iki tweet örnek olarak yazdırılmıştır.\n",
    "print(\"İlk tweet: \", dataset['Tweet'][0])\n",
    "print(\"İkinci tweet: \", dataset['Tweet'][1])\n",
    "\n",
    "# Tweet’lerde kullanıcı adları, bağlantılar, emojiler ve özel karakterlerin bulunduğu gözlemlenmiştir.\n",
    "\n",
    "# Pozitif ve negatif tweet’ler birleştirilerek iki ayrı kelime bulutu (WordCloud) oluşturulmuştur.\n",
    "positive_tweets = ' '.join(dataset[dataset['Sentiment'] == 4]['Tweet'].str.lower())\n",
    "negative_tweets = ' '.join(dataset[dataset['Sentiment'] == 0]['Tweet'].str.lower())\n",
    "\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(positive_tweets)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Pozitif Tweetler - Kelime Bulutu\")\n",
    "\n",
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(negative_tweets)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Negatif Tweetler - Kelime Bulutu\")\n",
    "\n",
    "# Kelime bulutlarından, pozitif ve negatif tweet’lerde geçen kelimelerin oldukça benzer olduğu görülmüştür.\n",
    "# Bu nedenle, benzer ifadeler içerse de doğru sınıflama yapabilecek güçlü bir sinir ağına ihtiyaç duyulduğu sonucuna varılmıştır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWc63brFQnK4"
   },
   "source": [
    "## Notebook Açıklaması\n",
    "\n",
    "1. **Kütüphanelerin İçe Aktarılması**  \n",
    "   - `pandas`: Veri okuma, işleme ve çerçeve (DataFrame) yapıları için.  \n",
    "   - `sklearn.model_selection.train_test_split`: Veriyi eğitim ve test kümelerine ayırmak için. (%80-%20)\n",
    "\n",
    "2. **Sütunların Tanımlanması**  \n",
    "   ```python\n",
    "   COLUMNS = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojGAUfLZcYf2",
    "outputId": "4a9bfaf5-c5b6-4708-f2c3-093851354889"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define variables\n",
    "COLUMNS = ['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Tweet']\n",
    "\n",
    "# Read dataset\n",
    "dataset = pd.read_csv('dataset.csv', names = COLUMNS, encoding = 'latin-1')\n",
    "print(colored(\"Columns: {}\".format(', '.join(COLUMNS)), \"yellow\"))\n",
    "\n",
    "# Remove extra columns\n",
    "print(colored(\"Useful columns: Sentiment and Tweet\", \"yellow\"))\n",
    "print(colored(\"Removing other columns\", \"red\"))\n",
    "dataset.drop(['Id', 'Date', 'Flag', 'User'], axis = 1, inplace = True)\n",
    "print(colored(\"Columns removed\", \"red\"))\n",
    "\n",
    "# Train test split\n",
    "print(colored(\"Splitting train and test dataset into 80:20\", \"yellow\"))\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['Tweet'], dataset['Sentiment'], test_size = 0.20, random_state = 100)\n",
    "train_dataset = pd.DataFrame({\n",
    "\t'Tweet': X_train,\n",
    "\t'Sentiment': y_train\n",
    "\t})\n",
    "print(colored(\"Train data distribution:\", \"yellow\"))\n",
    "print(train_dataset['Sentiment'].value_counts())\n",
    "test_dataset = pd.DataFrame({\n",
    "\t'Tweet': X_test,\n",
    "\t'Sentiment': y_test\n",
    "\t})\n",
    "print(colored(\"Test data distribution:\", \"yellow\"))\n",
    "print(test_dataset['Sentiment'].value_counts())\n",
    "print(colored(\"Split complete\", \"yellow\"))\n",
    "\n",
    "# Save train data\n",
    "print(colored(\"Saving train data\", \"yellow\"))\n",
    "\n",
    "train_dataset.to_csv('train.csv', index = False)\n",
    "print(colored(\"Train data saved to train.csv\", \"green\"))\n",
    "\n",
    "# Save test data\n",
    "print(colored(\"Saving test data\", \"yellow\"))\n",
    "test_dataset.to_csv('test.csv', index = False)\n",
    "print(colored(\"Test data saved to test.csv\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6rpo413RBVM"
   },
   "source": [
    "\n",
    "\n",
    "1. **Gerekli Kütüphanelerin İçe Aktarılması**  \n",
    "   - `re`, `nltk`, `numpy`: Metin işleme, doğal dil araçları ve sayısal işlemler için.  \n",
    "   - `PorterStemmer`, `WordNetLemmatizer`: Kök indirgeme ve lemmatizasyon için.\n",
    "\n",
    "2. **NLTK Kaynaklarının İndirilmesi**  \n",
    "   ```python\n",
    "   nltk.download('wordnet')\n",
    "   nltk.download('stopwords')\n",
    "\n",
    "\n",
    "##### Modelde kullanılmayacak sütunlar kaldırıldı\n",
    "##### Stopwordler kaldırıldı\n",
    "##### not gibi ifadeler için olumsuzluk genişletmesi yapıldı (don't -> do not)\n",
    "##### İsimler, özel karakter, nicknameler, URL adresleri, tek karakterli tokenlar kaldırıldı\n",
    "##### Lemmatization ile WordNet kullanarak kök halleri anlamlı hale getirildi\n",
    "##### Stemming ile çeşitli sondan ekler kaldırıldı, sözcükler kök haline doğru indirgendi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0efnwY3ReK5g",
    "outputId": "34780c3f-1b6b-43bb-f1cc-cdde7c415da6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from termcolor import colored\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import datasets\n",
    "print(\"Loading data\")\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Setting stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.remove(\"not\")\n",
    "\n",
    "# Function to expand tweet\n",
    "def expand_tweet(tweet):\n",
    "\texpanded_tweet = []\n",
    "\tfor word in tweet:\n",
    "\t\tif re.search(\"n't\", word):\n",
    "\t\t\texpanded_tweet.append(word.split(\"n't\")[0])\n",
    "\t\t\texpanded_tweet.append(\"not\")\n",
    "\t\telse:\n",
    "\t\t\texpanded_tweet.append(word)\n",
    "\treturn expanded_tweet\n",
    "\n",
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "\tdata['Clean_tweet'] = data['Tweet']\n",
    "\tprint(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
    "\tprint(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
    "\tprint(colored(\"Removing urls\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
    "\tprint(colored(\"Removing single characters\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
    "\tprint(colored(\"Tokenizing\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].str.split()\n",
    "\tprint(colored(\"Removing stopwords\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
    "\tprint(colored(\"Expanding not words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: expand_tweet(tweet))\n",
    "\tprint(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])\n",
    "\tprint(colored(\"Stemming the words\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])\n",
    "\tprint(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
    "\treturn data\n",
    "\n",
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "# Pre-processing the tweets\n",
    "print(colored(\"Processing train data\", \"green\"))\n",
    "train_data = clean_tweet(train_data, wordNetLemmatizer, porterStemmer)\n",
    "train_data.to_csv('clean_train.csv', index = False)\n",
    "print(colored(\"Train data processed and saved to clean_train.csv\", \"green\"))\n",
    "print(colored(\"Processing test data\", \"green\"))\n",
    "test_data = clean_tweet(test_data, wordNetLemmatizer, porterStemmer)\n",
    "test_data.to_csv('clean_test.csv', index = False)\n",
    "print(colored(\"Test data processed and saved to clean_test.csv\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T124WpQleVkX",
    "outputId": "ed00bb90-39c2-42eb-a5e3-98a8dd25368b"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiZ4Tic1TqvB"
   },
   "source": [
    "## Model Eğitimi Pipeline\n",
    "\n",
    " - Aşağıda DistilBERT tabanlı sınıflandırma modelini oluşturulmuştur.\n",
    " - Temizlenmiş tweetlerden %8'i kullanıldı.\n",
    " - DistilBERT tokenizasyon işlemi yapıldı.\n",
    " - Transfer Learning yapılarak DistilBERT modeli kullanıldı.\n",
    " (TFDistilBertForSequenceClassification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807,
     "referenced_widgets": [
      "235ea8c04f80448ea3a0cba537446fe5",
      "84fcf44f10284df1a8487ecc3c3880f2",
      "462c8dcf39964873ac4872fd235594b1",
      "13cf3dbc5b4b40b68a2b0e62042f8f9e",
      "3f1d93234a2e444aa1b026bdb5f20c67",
      "f94d318b64c74b0b97673baaf5e512b5",
      "681ed061162c470f9f8a140ed0241143",
      "d31644250b444c02b9cbfb1bfcaa2d76",
      "3a521a7467934d198376c5a37dc9b15c",
      "d7f0e61f80234aa18c06a0af05d87dc7",
      "65e79ac133364473bd8422c333dd58dc",
      "71dbcbe3765449bcac3c04cebce805e1",
      "ab30e0db4d7346b7a737eda4df82fb04",
      "bf6b166969c54d4c811121901a0346db",
      "66c785972218468bbd637a5f4ed15b84",
      "193a33a2a2b241fd81d02a14a4487d97",
      "44944807b9a04a51a2015bbecd9ea5a8",
      "8fafcdcb202f49faa8cdc85c388a30ca",
      "2000c517f22d40d89ac8e61c9b82ee69",
      "5cca99a193ea4b2d89aeb96d9059c51f",
      "2ebea2c84bad430cbbd65087dafe2fc7",
      "36ac147718804da3b5cc4bf94df4813a",
      "2069f7bd340a4ed993f7f7d62fa98795",
      "b851c93bbb1e42ae959bd057afe1b9ca",
      "c7c8548dd1d643f1b582c3bdd1b7feba",
      "6ca6b7d7bb0343d69acd9632fe76d848",
      "c149ebd753114678a1eb0b2afb9bce48",
      "95bb506b65874ca7b8e2bcfaab808d82",
      "12800b7f0b3c4b6ab8c544d96f912076",
      "c196fed2a50b40c49a908399c8653f79",
      "14954e238ca44f3eae2855ea0b9c995b",
      "6a9e8b2df41340d3a422c3dad315a31e",
      "433effbb0d624a4693ef25e572d65abb",
      "6254d38b11ee4b738a0230d17943c6d2",
      "9044ecc6e98c40868b3432e77f65bcd8",
      "2e6a1279ff034b668ff103d947e061cf",
      "0e6ea8debedb41d8aa01742af7419cc9",
      "e958d931db4d4095a1f1c5f40f627191",
      "d6c5c2db9f3742e393f64bc4082b55f3",
      "b9e73d6e6f5f47baac4f19632a02ecf3",
      "10f259c8b2ea449d86d20f2ce488ff1c",
      "173964c8d810402e9733e72eb32c624d",
      "ff0b4871ae97479996faf515f6c586ea",
      "019a9ffb359b41d6983ecba133eea803"
     ]
    },
    "id": "cW2vcflMegAv",
    "outputId": "c6ce3118-2561-4028-9527-7d83c793d5b0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from termcolor import colored\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Transformers (DistilBERT versiyonu)\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification,\n",
    "    create_optimizer\n",
    ")\n",
    "\n",
    "# --- 0) Mixed-Precision Etkinleştir ---\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# --- 1) VERİ YÜKLE & ÖN İŞLEME ---\n",
    "print(colored(\"Veriyi yükleyip ön işleme hazırlıyorum...\", \"yellow\"))\n",
    "full_train = pd.read_csv('clean_train.csv')\n",
    "\n",
    "train_data = (\n",
    "    full_train\n",
    "    .groupby('Sentiment', group_keys=False)\n",
    "    .sample(frac=0.08, random_state=123)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "train_data = train_data.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "train_data['Clean_tweet'] = train_data['Clean_tweet'].fillna('').astype(str)\n",
    "\n",
    "# --- 2) TRAIN/VAL SPLIT (stratify) ---\n",
    "print(colored(\"Stratified train/validation split oluşturuyorum...\", \"yellow\"))\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    train_data['Clean_tweet'].tolist(),\n",
    "    pd.get_dummies(train_data['Sentiment']).values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=pd.get_dummies(train_data['Sentiment']).values\n",
    ")\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# --- 3) TOKENİZASYON (DistilBERT) ---\n",
    "print(colored(\"DistilBERT tokenizer ile tokenleştime ve padding...\", \"yellow\"))\n",
    "MODEL_NAME = 'dbmdz/distilbert-base-turkish-cased'\n",
    "tokenizer  = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "max_len = 32   # Daha da kısıtlı sekans\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    X_train_texts,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    X_val_texts,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_len,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# --- 4) TF.DATA DATASET HAZIRLA ---\n",
    "print(colored(\"TF.data dataset oluşturuyorum...\", \"yellow\"))\n",
    "batch_size = 64  # Daha büyük batch ile daha az adım\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids':      train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask']\n",
    "    },\n",
    "    y_train\n",
    ")).shuffle(1000).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids':      val_encodings['input_ids'],\n",
    "        'attention_mask': val_encodings['attention_mask']\n",
    "    },\n",
    "    y_val\n",
    ")).batch(batch_size)\n",
    "\n",
    "# --- 5) DistilBERT-BASED SINIFLANDIRMA MODELİ ---\n",
    "print(colored(\"DistilBERT tabanlı sınıflandırma modelini yüklüyorum...\", \"yellow\"))\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# DistilBERT gövdesini dondur\n",
    "for layer in model.distilbert.transformer.layer:\n",
    "    layer.trainable = False\n",
    "\n",
    "# --- 6) DERLEYİCİ & OPTİMİZER ---\n",
    "print(colored(\"Modeli compile ediyorum...\", \"yellow\"))\n",
    "steps_per_epoch = len(train_dataset)\n",
    "num_train_steps = steps_per_epoch * 2  # 2 epoch planladık\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jrcm30cAeoK-",
    "outputId": "1ffc4f51-4788-4dba-8039-f9bde6af872c"
   },
   "outputs": [],
   "source": [
    "# --- 7) EĞİTİM ---\n",
    "print(colored(\"Eğitimi başlatıyorum...\", \"green\"))\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,      # Kritik: sadece 3 epoch\n",
    "    verbose=1\n",
    ")\n",
    "print(colored(\"Eğitim tamamlandı.\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMFLGV_nXGWS",
    "outputId": "bc45d967-2cbd-48cb-90ca-e9233e16a26d"
   },
   "outputs": [],
   "source": [
    "model.save('bert_model_tf', save_format='tf')\n",
    "print(colored(\"Model saved in TensorFlow SavedModel format to 'bert_model_tf'\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "daWj_vzjcMq7",
    "outputId": "6bb2db2b-f599-4107-d0e3-f0000a65c5a3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Eğitim ve doğrulama accuracy grafiği\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('accuracy_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Eğitim ve doğrulama loss grafiği\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
